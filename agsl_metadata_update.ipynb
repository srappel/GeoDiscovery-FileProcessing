{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGSL Metadata Update\n",
    "\n",
    "### Modified 2023-08-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from arcpy import metadata as md\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Update Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_metadata(dataset) -> tuple[str,md.Metadata,ET.Element]:\n",
    "    \"\"\"Gets a dataset's metadata\n",
    "    \n",
    "    Returns a tuple with three objects:\n",
    "    - XML as a single string\n",
    "    - arcpy.metadata.Metadata object\n",
    "    - xml.etree.ElementTree Element Object for the root element (<metadata>)\n",
    "    \n",
    "    :param str dataset: The path to the dataset as a string\n",
    "    :returns: a tuple with three object representations of the dataset's metadata\n",
    "    :rtype: tuple[str,md.Metadata,ET.Element]\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the md.Metadata object.\n",
    "    dataset_Metadata_object = md.Metadata(dataset)\n",
    "    \n",
    "    # isReadOnly test to make sure we can write to the object else error\n",
    "    if dataset_Metadata_object.isReadOnly is None: # This means that nothing was passed\n",
    "        #ERROR\n",
    "        print(\"A blank metadata object was created\")\n",
    "        return\n",
    "    elif dataset_Metadata_object.isReadOnly is True: # This means that a URI was passed, but it isn't a valid XML document\n",
    "        #ERROR\n",
    "        print(\"Not a valid URI\")\n",
    "        return\n",
    "    else:\n",
    "        # create an ET for the root\n",
    "        dataset_root_Element = ET.fromstring(dataset_Metadata_object.xml)\n",
    "        \n",
    "        # Return a tuple with the XML as a string, a Metadata object, and a Element object for the metadata (root) tag\n",
    "        return dataset_Metadata_object.xml, dataset_Metadata_object, dataset_root_Element    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_existing_identifier(rootElement, find_string) -> bool:\n",
    "    \"\"\" Check if the specified identifier exists in the metadata record\n",
    "    \n",
    "    Used in write_identifiers()\n",
    "    \n",
    "    returns a boolean (True or False) indicating if the string passed exists.\n",
    "    Uses the ET.findall() syntax to create a list of matches.\n",
    "    If there is 1 or more matches, returns true. Else returns false.\n",
    "    \n",
    "    :param xml.etree.ElementTree.Element rootElement: The root Element of the metadata\n",
    "    :param str find_string: a string using xml.etree.ElementTree.findall syntax\n",
    "    :returns: True if one or more of the element is found, False if none found.\n",
    "    :rtype: bool    \n",
    "    \"\"\"\n",
    "    dataset_identifier_list = rootElement.findall(find_string) # Returns a list\n",
    "    if len(dataset_identifier_list) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_title(metadata_root_element) -> str:\n",
    "    dataset_identifier_list = metadata_root_element.findall('.//resAltTitle') # Returns a list\n",
    "    if len(dataset_identifier_list) == 1:\n",
    "        return dataset_identifier_list[0].text\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_identifiers(rootElement, arkid, right_string, *identifiers) -> ET.Element:\n",
    "    \"\"\" Inserts the arkid as text property of specified identifiers in the metadata Element \n",
    "    \n",
    "    Returns the Element if successful.\n",
    "    Valid identifiers: \"mdFileID\", \"dataSetURI\", \"identCode\"\n",
    "    \n",
    "    TODO: Test to ensure the ark that is passed is valid? With a REGEX or maybe even a request?\n",
    "    TODO: Access constraints\n",
    "        dct_accessrights_s - XSLT looks at legalconstraints/other constraints\n",
    "        /MD_Metadata/identificationInfo/MD_DataIdentification/resourceConstraints[13]/MD_LegalConstraints/otherConstraints/gco:CharacterString\n",
    "    TODO: Update the dataSetURI to use the download URL rather than the application URL\n",
    "        \n",
    "    :param xml.etree.Element rootElement: The root Element of the metadata\n",
    "    :param str arkid: the full arkid as a string (e.g. '77981/gmgsfq9q48t')\n",
    "    :param str right_string: 'public', 'restricted-uwm', 'restricted-uw-system' # Matches directory on the Apache server\n",
    "    :param str *identifiers: a strig indicating which identifier to target. Conrolled Vocab.\n",
    "    :returns: The root Element (metadata), having been updated.\n",
    "    :rtype: xml.etree.ElementTree.Element\n",
    "    \"\"\"   \n",
    "    \n",
    "    ark_URI = 'https://geodiscovery.uwm.edu/ark:-' + arkid.replace('/','-')\n",
    "    \n",
    "    alt_title = get_alt_title(rootElement)\n",
    "    download_URI = 'https://geodata.uwm.edu/' + right_string + '/' + alt_title + '.zip'\n",
    "    \n",
    "    for identifier in identifiers:    \n",
    "        if identifier not in [\"mdFileID\",\"dataSetURI\",\"identCode\"]:\n",
    "            print(\"Valid options for identifier are 'mdFileID', 'dataSetURI', or 'identCode'.\")\n",
    "            return\n",
    "        elif identifier == 'identCode':\n",
    "            identifier = r'.//citId/identCode'\n",
    "            if check_if_existing_identifier(rootElement, identifier) is True:\n",
    "                dataset_identCode = rootElement.find(identifier)\n",
    "                dataset_identCode.text = ark_URI\n",
    "            else:\n",
    "                dataset_idCitation = rootElement.find('.dataIdInfo/idCitation')\n",
    "                citId_Element = ET.SubElement(dataset_idCitation, 'citId', xmls=\"\")\n",
    "                identCode_Element = ET.SubElement(citId_Element, 'identCode')\n",
    "                identCode_Element.text = ark_URI\n",
    "        else:\n",
    "            if check_if_existing_identifier(rootElement, identifier) is True:\n",
    "                dataset_identifier = rootElement.find(identifier)\n",
    "                if identifier == 'mdFileID':\n",
    "                    dataset_identifier.text = f'ark:/77981/{arkid.split(\"/\")[1]}'\n",
    "                elif identifier == 'dataSetURI':\n",
    "                    dataset_identifier.text = download_URI\n",
    "            else:\n",
    "                identifier_Element = ET.SubElement(rootElement, identifier)\n",
    "                if identifier == 'mdFileID':\n",
    "                    identifier_Element.text = f'ark:/77981/{arkid.split(\"/\")[1]}'\n",
    "                elif identifier == 'dataSetURI':\n",
    "                    identifier_Element.text = download_URI \n",
    "    print(f'The URI for the dataset is {ark_URI}')\n",
    "    print(f'The Download URL for the dataset is {download_URI}')\n",
    "    return rootElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_metadata_export(dataset, md_outputdir=None, md_filename=None) -> tuple[Path,Path]:\n",
    "    \"\"\"Exports an ISO and FGDC format XML file for the dataset.\n",
    "    \n",
    "    :param str dataset: The path to the dataset as a string\n",
    "    :param str md_outputdir: The directory to store the 2 new files. Default is the parent directory, or grandparent if parent is a FileGeodatabase.\n",
    "    :param str md_filename: A AGSL format filename (e.g. geography_theme_year) to use as the metadata filename. \n",
    "    :returns: a tuple containing the Path to the ISO and FGDC XML Files\n",
    "    :rtype: tuple[Path,Path]\n",
    "    \"\"\"\n",
    "    # Create the Metadata Object\n",
    "    dataset_md = md.Metadata(dataset)\n",
    "    \n",
    "    # If the md_outputdir optional argument is specified, use it was the output directory\n",
    "    # Otherwise default to the parent directory of the dataset.\n",
    "    # If the parent directory is a FileGeodatabase, use the Grandparent (parent dir of the Geodatabase)\n",
    "    if md_outputdir is None:\n",
    "        if Path(dataset).parent.suffix == \".gdb\":\n",
    "            parent = Path(dataset).parent.parent\n",
    "        elif Path(dataset).parent.is_dir() is False:\n",
    "            print(\"Parent is not a dir!\")\n",
    "            return\n",
    "        else:\n",
    "            parent = Path(dataset).parent\n",
    "    else:\n",
    "        if Path(md_outputdir).is_dir():\n",
    "            parent = Path(md_outputdir)\n",
    "        else:\n",
    "            print(\"specified output directory is not a valid directory!\")\n",
    "            return\n",
    "        \n",
    "    # If the md_filename optional argument is specified, use it as the filename for the new metadata.\n",
    "    # Otherwise, use the dataset's filename (the Path.stem) as the filename to prefix the _ISO or _FGDC suffix.\n",
    "    if md_filename is None: # Default\n",
    "        output_ISO_Path = parent / f'{Path(dataset).stem}_ISO.xml'\n",
    "        output_FGDC_Path = parent / f'{Path(dataset).stem}_FGDC.xml'\n",
    "    else:\n",
    "        output_ISO_Path = parent / f'{md_filename}_ISO.xml'\n",
    "        output_FGDC_Path = parent / f'{md_filename}_FGDC.xml'\n",
    "    \n",
    "    # Print the ISO path to the console and export the metadata\n",
    "    print(output_ISO_Path)\n",
    "    dataset_md.exportMetadata(output_ISO_Path, 'ISO19139_GML32', metadata_removal_option='REMOVE_ALL_SENSITIVE_INFO')\n",
    "    \n",
    "    # Print the FGDC path to the console and export the metadata\n",
    "    print(output_FGDC_Path)\n",
    "    dataset_md.exportMetadata(output_FGDC_Path, 'FGDC_CSDGM', metadata_removal_option='REMOVE_ALL_SENSITIVE_INFO')\n",
    "    \n",
    "    return output_ISO_Path, output_FGDC_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchForID(text) -> list:\n",
    "    \"\"\"Searches for an arkid in the response string\n",
    "    \n",
    "    Used in mintArk()\n",
    "    \n",
    "    Returns a list with three items:\n",
    "    - [0] the full arkid\n",
    "    - [1] the Name Authority Number\n",
    "    - [2] the Assigned Name\n",
    "    \n",
    "    :param str text: The returned text from the minter\n",
    "    :returns: a list containing the regex groups\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    arkregex = re.compile(r\"(\\d{5})\\/(\\w{11})\")\n",
    "    arkid = arkregex.search(text)\n",
    "    return arkid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noid stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mintArk(minter) -> dict:\n",
    "    \"\"\"Send the request to the minter. Catch connection errors.\n",
    "    \n",
    "    Returns a dictionary with three items:\n",
    "    - \"arc\": the full arkid\n",
    "    - \"nan\": the Name Authority Number\n",
    "    - \"asn\": the Assigned Name\n",
    "    \n",
    "    :param str minter: The URL for the minter\n",
    "    :returns dict arkDict: a dictionary with regex results\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(minter)\n",
    "    except:\n",
    "        print(\"There was a connection error! Check the URL you used to request the id!\")\n",
    "        return\n",
    "    # Check the status code the minter returns. It should be 200 if the request was completed.\n",
    "    if r.status_code != 200:\n",
    "        print(\"There was a non-200 status code from the minter: \" + r.status_code)\n",
    "        return\n",
    "    # If the status code is 200, then grab the text, run it through the searchForID function to get a string with the arkID\n",
    "    else:\n",
    "        minter_text = r.text\n",
    "        regex_result = searchForID(minter_text) # This should return a list\n",
    "        arkDict = {}\n",
    "        arkDict.update({\"arc\": regex_result[0]})\n",
    "        arkDict.update({\"nan\": regex_result[1]})\n",
    "        arkDict.update({\"asn\": regex_result[2]})\n",
    "        return arkDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(metadata) -> str:\n",
    "    while metadata.__class__ == arcpy.metadata.Metadata:\n",
    "        root_Element = ET.fromstring(metadata.xml)\n",
    "        try:\n",
    "            tmBegin = root_Element.find('.//tmBegin').text\n",
    "            tmEnd = root_Element.find('.//tmEnd').text\n",
    "            date_when = f'{tmBegin}/{tmEnd}'\n",
    "            return date_when\n",
    "        except:\n",
    "            date_when = root_Element.find('.//tmPosition').text\n",
    "            return date_when\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bind_params(metadata, right_string) -> dict:\n",
    "    '''Create Bind Parameters\n",
    "    Defines the DC Kernel Metadata objects that will bind to the Ark using the NOID binder.\n",
    "    Input must be a arcpy.metadata.Metadata object.\n",
    "    '''    \n",
    "    if metadata.__class__ == arcpy.metadata.Metadata:\n",
    "        root_Element = ET.fromstring(metadata.xml)\n",
    "        mdfileid = root_Element.find('mdFileID').text\n",
    "        ark_URI = 'https://www.geodiscovery.uwm.edu/' + mdfileid.replace('/','-')\n",
    "        date_when = get_date_range(metadata)\n",
    "        time_now = datetime.now().replace(microsecond=0).isoformat()\n",
    "        parameter_dictionary = {\n",
    "            \"who\": f'{metadata.credits}',\n",
    "            \"what\": f'{metadata.title}',\n",
    "            \"when\": f'{date_when}',\n",
    "            \"where\": f'{ark_URI}',\n",
    "            \"meta-who\": \"University of Wisconsin-Milwaukee Libraries\",\n",
    "            \"meta-when\": f'{time_now}',\n",
    "            \"rights\": f'{right_string}'  \n",
    "        }\n",
    "        return parameter_dictionary\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bind_request(arkid, bind_params) -> str:\n",
    "    '''Construct Noid Bind request string\n",
    "    This function will take an Ark ID and bind parameters and format the request data needed for the bind POST request.\n",
    "    Returns a string.\n",
    "    :param str arkid: from the metadata formatted like '77981/gmgsbr8mf8v'\n",
    "    :param dict bind_params: A dictionary or DC-like parameters generated by create_bind_params() \n",
    "    :return str param_string: The data that will get passed to the bind request. It's noid bind set commands separated by newlines.\n",
    "    '''\n",
    "    if not arkid.__class__ == str and bind_params.__class__ == dict:\n",
    "        print(\"at least one function argument is the wrong class\")\n",
    "        return\n",
    "    if not len(bind_params) >= 1:\n",
    "        print(\"the bind_params are a dict but it's empty.\")\n",
    "        return\n",
    "    \n",
    "    param_string = ''\n",
    "    for key, value in bind_params.items():\n",
    "        param_string = param_string + f'bind set {arkid} {key} \"{value}\"' + '\\n'\n",
    "    \n",
    "    return param_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bindArk(metadata, arkid, right_string, binder) -> requests.models.Response:\n",
    "    '''Bind Ark\n",
    "    Sends a request to NOID to bind some parameters to an Ark ID\n",
    "    :param str bind_url: The URL for the post request. Since we're adding multiple parameters, we use a - as it's only HTTP arg and then it expects a newline separated list of NOID commands\n",
    "    :param arcpy.metadata.Metadata metadata: an ArcGIS Metadata object\n",
    "    :param str right_string: 'public', 'restricted-uw-system', 'restricted-uwm'\n",
    "    :param str arkid: the ArkID formatted like '77981/gmgsbr8mf8v'\n",
    "    :return requests.models.Response response: A response object from the requests lib    \n",
    "    '''\n",
    "    bind_params = create_bind_params(metadata, right_string)\n",
    "    bind_params_commands = construct_bind_request(arkid, bind_params)\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(binder, data=bind_params_commands)\n",
    "    except:\n",
    "        print(\"There was a connection error! Check the URL you used to bind the id!\")\n",
    "        return\n",
    "    # Check the status code the minter returns. It should be 200 if the request was completed.\n",
    "    if r.status_code != 200:\n",
    "        print(\"There was a non-200 status code from the binder: \" + r.status_code)\n",
    "        return\n",
    "    # If the status code is 200, then grab the text, run it through the searchForID function to get a string with the arkID\n",
    "    else:\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agsl_metadata_update(dataset,\n",
    "                         minter=r'https://digilib-dev.uwm.edu/noidu_gmgs?mint+1',\n",
    "                         right_string ='public',\n",
    "                         binder = r'https://digilib-dev.uwm.edu/noidu_gmgs?-',\n",
    "                         md_output_directory=None,\n",
    "                         md_filename=None\n",
    "                        ) -> tuple[md.Metadata, str]:\n",
    "    \"\"\"AGSL Metadata Update\n",
    "    \n",
    "    Take a dataset, an optional minterURL, an optional rights string, an optional binder URL\n",
    "    an optional output directory for metadata, and optional custom filename for the metadata outputs.\n",
    "    \n",
    "    Inserts the minted arkID into the metadata, exports ISO and FGDC format metadata, and then binds some core metadata parameters to the ArkID using NOID.\n",
    "    \n",
    "    :param str dataset: the path to the dataset as a string\n",
    "    :param str minter: The URL for the minter\n",
    "    :param str md_outputdir: The directory to store the 2 new files. Default is the parent directory, or grandparent if parent is a FileGeodatabase.\n",
    "    :param str md_filename: A AGSL format filename (e.g. geography_theme_year) to use as the metadata filename.\n",
    "    :returns: a tuple with the arcpy.metadata.Metadata object and a string representation of the arkid\n",
    "    :rtype: tuple[arcpy.metadata.Metadata, str]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mint the Ark ID\n",
    "    try:\n",
    "        ark = mintArk(minter)\n",
    "        print(f\"The full ARK is: {ark['arc']}\")\n",
    "        print(f\"The Name Authority # is: {ark['nan']}\")\n",
    "        print(f\"The Asssigned Name is: {ark['asn']}\")\n",
    "        arkid = ark['arc']\n",
    "    except:\n",
    "        print(\"There was a problem with the mint request\")\n",
    "        return\n",
    "    \n",
    "    # Access and write the item Metadata\n",
    "    try:    \n",
    "        xml_text, dataset_Metadata, dataset_Element = get_dataset_metadata(dataset)\n",
    "        dataset_Metadata.xml = ET.tostring(write_identifiers(dataset_Element,arkid, right_string, 'identCode', 'mdFileID', 'dataSetURI'),encoding='unicode')\n",
    "        dataset_Metadata.save()\n",
    "    except:\n",
    "        print(\"there was a problem getting the dataset metadata\")\n",
    "        return\n",
    "    \n",
    "    # Export Metadata to FGDC and ISO formats\n",
    "    try:\n",
    "        dual_metadata_export(dataset, md_output_directory, md_filename)\n",
    "    except:\n",
    "        print(\"there was a problem exporting the metadata\")\n",
    "        return\n",
    "    \n",
    "    # Bind the ArkID\n",
    "    try:\n",
    "        bind_response = bindArk(dataset_Metadata, arkid, right_string, binder)\n",
    "    except:\n",
    "        print(\"There was a problem with the bind request\")\n",
    "        return\n",
    "    \n",
    "    return dataset_Metadata, arkid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing and testing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_dirs(rootdir) -> list[tuple[Path,int]]:\n",
    "    '''List All Directories\n",
    "    - Lists all directories in a given root directory\n",
    "    - Includes File Geodatabases with a .gdb extension\n",
    "    - Does not include Feature Datasets within a File Geodatabase\n",
    "    - Returns a list of pathlib.Path type directory paths\n",
    "    '''\n",
    "    rootdir = Path(rootdir)\n",
    "    all_directories = []\n",
    "    for path in sorted(rootdir.rglob(\"*\")):\n",
    "        if path.is_dir():\n",
    "            depth = len(path.relative_to(rootdir).parts)\n",
    "            path_tuple = (path, depth)\n",
    "            all_directories.append(path_tuple)\n",
    "    return all_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_type(rootdir) -> int:\n",
    "    '''Test Dataset Type\n",
    "    - Iterates through all subpaths\n",
    "    - Checks for certain file extensions to determine type\n",
    "    - Returns an integer for type:\n",
    "        - 0: Error\n",
    "        - 1: Shapefile\n",
    "        - 2: FileGeodatabase\n",
    "        - 3: ArcGRID Raster\n",
    "        - 4: Other/Mutliple\n",
    "    '''    \n",
    "    rootdir = Path(rootdir)\n",
    "    \n",
    "    gdb_count = 0\n",
    "    shp_count = 0\n",
    "    raster_count = 0\n",
    "    \n",
    "    for path in Path(rootdir).rglob(\"*\"):\n",
    "        if path.suffix == \".gdb\":\n",
    "            gdb_count += 1\n",
    "            continue\n",
    "        elif path.suffix == \".shp\":\n",
    "            shp_count += 1\n",
    "        elif path.suffix == \".adf\":\n",
    "            raster_count += 1\n",
    "    \n",
    "    if gdb_count == 0 and shp_count == 0 and raster_count == 0:\n",
    "        return 0\n",
    "    elif gdb_count == 0 and shp_count == 1 and raster_count == 0:\n",
    "        return 1\n",
    "    elif gdb_count == 1 and shp_count == 0 and raster_count == 0:\n",
    "        return 2\n",
    "    elif gdb_count == 0 and shp_count == 0 and raster_count >= 1:\n",
    "        # Note that there might be more than one .adf file!\n",
    "        return 3\n",
    "    elif (gdb_count + shp_count + raster_count) >= 1:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataset_from_directory(directory) -> Path:\n",
    "    '''Fetech Dataset From Directory\n",
    "    Pass this function a direcotry Path or path as string and it will test what type of dataset it is and then return a Path to the dataset.\n",
    "    :param pathlib.Path/str directory: a path to the directory as a pathlib.Path object or as a string\n",
    "    :return pathlib.Path dataset: a pathlib.Path to the dataset that can be used in the arcpy.metadata.Metadata constructor\n",
    "    '''\n",
    "    directory = Path(directory) # This ensures that the directory is a Path (A Path(Path) is still a Path)\n",
    "    dataset_type = test_dataset_type(directory)\n",
    "    if dataset_type != 0: # 0 would mean there is an error\n",
    "        if dataset_type == 1: # Shapefile\n",
    "            dataset = next(Path(directory).rglob(\"*.shp\"))\n",
    "        elif dataset_type == 2: # FileGeodatabase\n",
    "            geodatabase = next(Path(directory).rglob(\"*.gdb\")) # Path Representation of the geodatabase\n",
    "            arcpy.env.workspace = str(geodatabase) # This can't be a Path, it has to be a path as string.\n",
    "            feature_dataset_list = arcpy.ListDatasets(\"*\",\"feature\")\n",
    "            if not len(feature_dataset_list) > 1:\n",
    "                dataset = Path(geodatabase) / feature_dataset_list[0]\n",
    "            else:\n",
    "                return\n",
    "        elif dataset_type == 3: # Raster Dataset... ArcGrid only for now.\n",
    "            arcpy.env.workspace = str(directory) # This can't be a Path, it has to be a path as string.\n",
    "            raster_dataset_list = arcpy.ListRasters(\"*\")\n",
    "            if not len(raster_dataset_list) > 1:\n",
    "                dataset = directory / raster_dataset_list[0]\n",
    "            else:\n",
    "                return  \n",
    "        elif dataset_type == 4:\n",
    "            return        \n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    if md.Metadata(dataset).__class__ == arcpy.metadata.Metadata:\n",
    "        return dataset\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run it on the test fixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\DoorCounty_Lighthouses_2010_UW\\DoorCounty_Lighthouses_2010.shp:\n",
      "The full ARK is: 77981/gmgsqv3c348\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgsqv3c348\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgsqv3c348\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\DoorCounty_Lighthouses_2010_UW\\DoorCounty_Lighthouses_2010_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\DoorCounty_Lighthouses_2010_UW\\DoorCounty_Lighthouses_2010_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgsqv3c348\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\Milwaukee_AldermanicWards_1896-1901\\Milwaukee_AldermanicWards_1896-1901.shp:\n",
      "The full ARK is: 77981/gmgsm61bp34\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgsm61bp34\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgsm61bp34\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\Milwaukee_AldermanicWards_1896-1901\\Milwaukee_AldermanicWards_1896-1901_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\Milwaukee_AldermanicWards_1896-1901\\Milwaukee_AldermanicWards_1896-1901_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgsm61bp34\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_Cadastral_2020\\MilwaukeeCounty_Cadastral_2020.gdb\\MilwaukeeCounty_Cadastral_2020:\n",
      "The full ARK is: 77981/gmgsgf0mv9t\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgsgf0mv9t\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgsgf0mv9t\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_Cadastral_2020\\MilwaukeeCounty_Cadastral_2020_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_Cadastral_2020\\MilwaukeeCounty_Cadastral_2020_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgsgf0mv9t\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_MCTSStops_2018\\MilwaukeeCounty_MCTSStops_2018.shp:\n",
      "The full ARK is: 77981/gmgsbn9x252\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgsbn9x252\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgsbn9x252\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_MCTSStops_2018\\MilwaukeeCounty_MCTSStops_2018_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_MCTSStops_2018\\MilwaukeeCounty_MCTSStops_2018_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgsbn9x252\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_NationalElevationDataset_1999\\ned_34599980:\n",
      "The full ARK is: 77981/gmgs6w96816\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgs6w96816\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgs6w96816\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_NationalElevationDataset_1999\\ned_34599980_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\MilwaukeeCounty_NationalElevationDataset_1999\\ned_34599980_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgs6w96816\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\PortlandMetro_Freeways_2000\\PortlandMetro_Freeways_2000.shp:\n",
      "The full ARK is: 77981/gmgs348gf78\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgs348gf78\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgs348gf78\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\PortlandMetro_Freeways_2000\\PortlandMetro_Freeways_2000_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\PortlandMetro_Freeways_2000\\PortlandMetro_Freeways_2000_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgs348gf78\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\WaukeshaCounty_DEM_1999\\wauk_dem:\n",
      "The full ARK is: 77981/gmgszc7rt20\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgszc7rt20\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgszc7rt20\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\WaukeshaCounty_DEM_1999\\wauk_dem_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\WaukeshaCounty_DEM_1999\\wauk_dem_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgszc7rt20\n",
      "\n",
      "Processing: C:\\Users\\srappel\\Desktop\\Test Fixture Data\\Wisconsin_IceAgeTrail_2019\\Wisconsin_IceAgeTrail_2019.shp:\n",
      "The full ARK is: 77981/gmgstm7208m\n",
      "The Name Authority # is: 77981\n",
      "The Asssigned Name is: gmgstm7208m\n",
      "The URI for the dataset is https://www.geodiscovery.uwm.edu/ark:-77981-gmgstm7208m\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\Wisconsin_IceAgeTrail_2019\\Wisconsin_IceAgeTrail_2019_ISO.xml\n",
      "C:\\Users\\srappel\\Desktop\\Test Fixture Data\\Wisconsin_IceAgeTrail_2019\\Wisconsin_IceAgeTrail_2019_FGDC.xml\n",
      "NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+77981/gmgstm7208m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_directory in list_all_dirs(r\"C:\\\\Users\\\\srappel\\\\Desktop\\\\Test Fixture Data\"): # Gets all the directories\n",
    "    if dataset_directory[1] == 1: # Filters it to only children of the root directory\n",
    "        dataset = fetch_dataset_from_directory(dataset_directory[0]) # fetches the dataset path\n",
    "        if not dataset is None: \n",
    "            print(f'Processing: {dataset}:')\n",
    "            arkid = agsl_metadata_update(dataset)[1] # updates the metadata and returns the arkid\n",
    "            print(f'NOID URL is: https://digilib-dev.uwm.edu/noidu_gmgs?get+{arkid}' + '\\n')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
